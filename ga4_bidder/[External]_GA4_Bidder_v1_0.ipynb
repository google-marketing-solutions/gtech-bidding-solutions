{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLTbIpHZ0KaV"
      },
      "source": [
        "# **GA4 Bidder**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################################################\n",
        "#\n",
        "#  Copyright 2021 Google Inc.\n",
        "#\n",
        "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#  you may not use this file except in compliance with the License.\n",
        "#  You may obtain a copy of the License at\n",
        "#\n",
        "#      https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#  Unless required by applicable law or agreed to in writing, software\n",
        "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#  See the License for the specific language governing permissions and\n",
        "#  limitations under the License.\n",
        "#\n",
        "# This solution, including any related sample code or data, is made available\n",
        "# on an “as is,” “as available,” and “with all faults” basis, solely for\n",
        "# illustrative purposes, and without warranty or representation of any kind.\n",
        "# This solution is experimental, unsupported and provided solely for your\n",
        "# convenience. Your use of it is subject to your agreements with Google, as\n",
        "# applicable, and may constitute a beta feature as defined under those\n",
        "# agreements.  To the extent that you make any data available to Google in\n",
        "# connection with your use of the solution, you represent and warrant that you\n",
        "# have all necessary and appropriate rights, consents and permissions to permit\n",
        "# Google to use and process that data.  By using any portion of this solution,\n",
        "# you acknowledge, assume and accept all risks, known and unknown, associated\n",
        "# with its usage, including with respect to your deployment of any portion of\n",
        "# this solution in your systems, or usage in connection with your business,\n",
        "# if at all.\n",
        "###########################################################################"
      ],
      "metadata": {
        "id": "IKnMPrQcDgHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch8FoI3YrVzO"
      },
      "source": [
        "# 1) Define Feature Set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1) Imports, Globals, & Helper Functions"
      ],
      "metadata": {
        "id": "R__B9Nt9EPtb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIOTfVBOm38n",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Load libraries and authenticate colab for BQ\n",
        "!pip install --upgrade -q gspread | grep -v 'already satisfied'\n",
        "!pip install gspread_formatting | grep -v 'already satisfied'\n",
        "\n",
        "import gspread\n",
        "import math\n",
        "import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.auth import default\n",
        "from gspread_formatting import *\n",
        "from multiprocessing import Process\n",
        "import ipywidgets as widgets\n",
        "auth.authenticate_user()\n",
        "bigquery.USE_LEGACY_SQL = False\n",
        "\n",
        "dc = {}\n",
        "\n",
        "def query_to_df(query, project_id, dialect='standard'):\n",
        "  \"\"\"Returns a dataframe w/ the result of query.\n",
        "  \"\"\"\n",
        "\n",
        "  # Run the query and save to a dataframe\n",
        "  df = pd.read_gbq(query, project_id=project_id, dialect=dialect)\n",
        "\n",
        "  return df\n",
        "\n",
        "def build_eda_query(label_query, ga_project_id, ga_dataset, ga_data, filters):\n",
        "  return f\"\"\"\n",
        "    WITH\n",
        "\n",
        "    {label_query},\n",
        "\n",
        "    event_list as (\n",
        "      select distinct user_pseudo_id, event_name,\n",
        "      CASE\n",
        "      WHEN event_name in ('ad_click','ad_exposure','ad_impression','ad_query','ad_reward','adunit_exposure','app_clear_data','app_remove','app_store_refund','app_store_subscription_cancel','app_store_subscription_convert','app_store_subscription_renew','app_update','dynamic_link_app_open','dynamic_link_app_update','error','firebase_campaign','firebase_in_app_message_dismiss','firebase_in_app_message_impression','first_open','first_visit','in_app_purchase','notification_dismiss','notification_foreground','notification_open','notification_receive','notification_send','os_update','screen_view','session_start','user_engagement','click','file_download','first_visit','page_view','scroll','session_start','user_engagement','video_complete','video_progress','video_start','view_search_results')\n",
        "        THEN 'standard'\n",
        "      WHEN event_name in ('earn_virtual_currency','join_group','login','purchase','refund','search','select_content','share','sign_up','spend_virtual_currency','tutorial_begin','tutorial_complete','add_payment_info','add_shipping_info','add_to_cart','add_to_wishlist','begin_checkout','generate_lead','remove_from_cart','select_item','select_promotion','view_cart','view_item_list','view_promotion','level_end','level_start','level_up','post_score','unlock_achievement')\n",
        "      THEN 'recommended'\n",
        "      ELSE 'custom' END\n",
        "      AS event_type\n",
        "      from `{ga_project_id}.{ga_dataset}.{ga_data}`\n",
        "      where {filters}\n",
        "      )\n",
        "\n",
        "    select event_name, event_type, count(distinct user_pseudo_id) as total_users, count(distinct case when label=1 then user_pseudo_id else null end) as total_converters,\n",
        "    count(distinct case when label=1 then user_pseudo_id else null end)/count(distinct user_pseudo_id) as  share_converters,\n",
        "    from\n",
        "    (select a.*, b.label from event_list a join visitors_labeled b on a.user_pseudo_id=b.user_pseudo_id)\n",
        "    group by 1,2\n",
        "    order by 5 desc\"\"\"\n",
        "\n",
        "def create_build_feature_set_sql(label_query, ga_project_id, ga_dataset, ga_data, filters, end_date, event_level_query):\n",
        "  return f\"\"\"\n",
        "    WITH\n",
        "\n",
        "    {label_query},\n",
        "\n",
        "\n",
        "  visitor_city AS (\n",
        "      SELECT\n",
        "        user_pseudo_id, metro AS dma\n",
        "      FROM (\n",
        "        SELECT c.user_pseudo_id,c.metro, ROW_NUMBER() OVER (PARTITION BY c.user_pseudo_id ORDER BY visits DESC) AS row_num\n",
        "        FROM (\n",
        "          SELECT a.user_pseudo_id, geo.metro AS metro, COUNT(*) AS visits\n",
        "          FROM `{ga_project_id}.{ga_dataset}.{ga_data}` a\n",
        "          left join visitors_labeled b\n",
        "          on a.user_pseudo_id = b.user_pseudo_id\n",
        "          where {filters}\n",
        "          and (a.event_timestamp < IFNULL(first_transaction_session, 0)\n",
        "          or first_transaction_session is null)\n",
        "          GROUP BY user_pseudo_id,metro ) c )\n",
        "    WHERE row_num = 1 ),\n",
        "\n",
        "  visitor_common_daypart AS (\n",
        "    SELECT user_pseudo_id, daypart\n",
        "    FROM (\n",
        "      SELECT user_pseudo_id, daypart, ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY pageviews DESC) AS row_num\n",
        "      FROM (\n",
        "        SELECT\n",
        "          user_pseudo_id,\n",
        "          CASE WHEN hour_of_day_localized >= 1 AND hour_of_day_localized < 6 THEN 'night_1_6'\n",
        "          WHEN hour_of_day_localized >= 6 AND hour_of_day_localized < 11 THEN 'morning_6_11'\n",
        "          WHEN hour_of_day_localized >= 11 AND hour_of_day_localized < 14 THEN 'lunch_11_14'\n",
        "          WHEN hour_of_day_localized >= 14 AND hour_of_day_localized < 17 THEN 'afternoon_14_17'\n",
        "          WHEN hour_of_day_localized >= 17 AND hour_of_day_localized < 19 THEN 'dinner_17_19'\n",
        "          WHEN hour_of_day_localized >= 19 AND hour_of_day_localized < 22 THEN 'evening_19_23'\n",
        "          WHEN hour_of_day_localized >= 22 OR hour_of_day_localized = 0 THEN 'latenight_23_1'\n",
        "          END AS daypart,\n",
        "          SUM(pageviews) AS pageviews\n",
        "        FROM (\n",
        "          SELECT a.user_pseudo_id, EXTRACT(HOUR\n",
        "            FROM (\n",
        "                CASE WHEN c.dst = 1 AND event_date BETWEEN '20170312' AND '20171105' THEN TIMESTAMP_ADD(TIMESTAMP_MICROS(event_timestamp), INTERVAL c.timezone+1 HOUR)\n",
        "                WHEN c.dst = 1 AND event_date BETWEEN '20180311' AND '20181104' THEN TIMESTAMP_ADD(TIMESTAMP_MICROS(event_timestamp), INTERVAL c.timezone+1 HOUR)\n",
        "                WHEN c.dst = 1 AND event_date BETWEEN '20190310' AND '20191103' THEN TIMESTAMP_ADD(TIMESTAMP_MICROS(event_timestamp), INTERVAL c.timezone+1 HOUR)\n",
        "                ELSE TIMESTAMP_ADD(TIMESTAMP_MICROS(event_timestamp), INTERVAL c.timezone HOUR)\n",
        "                END )) AS hour_of_day_localized,\n",
        "                case when event_name = 'page_view' then 1 else 0 end as pageviews\n",
        "          FROM `{ga_project_id}.{ga_dataset}.{ga_data}` a\n",
        "          LEFT JOIN ( SELECT state_name, MAX(CASE WHEN state_name = 'Oregon' THEN -8 ELSE timezone END) AS timezone, MAX(dst) AS dst\n",
        "            FROM (SELECT states.*\n",
        "                      FROM UNNEST ([STRUCT(\"Alaska\" as state_name, -9 as timezone, 1 as dst),\n",
        "                        STRUCT(\"American Samoa\" as state_name, -10 as timezone, 0 as dst),\n",
        "                        STRUCT(\"Hawaii\" as state_name, -10 as timezone, 0 as dst),\n",
        "                        STRUCT(\"California\" as state_name, -7 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Idaho\" as state_name, -7 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Nevada\" as state_name, -8 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Oregon\" as state_name, -8 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Washington\" as state_name, -8 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Arizona\" as state_name, -7 as timezone, 0 as dst),\n",
        "                        STRUCT(\"Colorado\" as state_name, -7 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Kansas\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Montana\" as state_name, -7 as timezone, 1 as dst),\n",
        "                        STRUCT(\"North Dakota\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Nebraska\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"New Mexico\" as state_name, -7 as timezone, 1 as dst),\n",
        "                        STRUCT(\"South Dakota\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Texas\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Utah\" as state_name, -7 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Wyoming\" as state_name, -7 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Alabama\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Arkansas\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Florida\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Iowa\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Illinois\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Indiana\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Kentucky\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Louisiana\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Michigan\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Minnesota\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Missouri\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Mississippi\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Oklahoma\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Tennessee\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Wisconsin\" as state_name, -6 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Connecticut\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"District of Columbia\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Delaware\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Georgia\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Massachusetts\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Maryland\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Maine\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"North Carolina\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"New Hampshire\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"New Jersey\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"New York\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Ohio\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Pennsylvania\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Rhode Island\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"South Carolina\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Virginia\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Vermont\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"West Virginia\" as state_name, -5 as timezone, 1 as dst),\n",
        "                        STRUCT(\"Puerto Rico\" as state_name, -4 as timezone, 0 as dst),\n",
        "                        STRUCT(\"Virgin Islands\" as state_name, -4 as timezone, 0 as dst)]) states)\n",
        "            GROUP BY state_name) c\n",
        "          ON a.geo.region = c.state_name\n",
        "          left join visitors_labeled b\n",
        "          on a.user_pseudo_id = b.user_pseudo_id\n",
        "          where {filters}\n",
        "          and (a.event_timestamp < IFNULL(first_transaction_session, 0)\n",
        "          or first_transaction_session is null)\n",
        "          )\n",
        "        GROUP BY 1, 2 ) )\n",
        "    WHERE row_num = 1 ),\n",
        "\n",
        "  visitor_common_day AS (\n",
        "    SELECT user_pseudo_id, case when day = 1 then \"Sunday\"\n",
        "    when day = 2 then \"Monday\"\n",
        "    when day = 3 then \"Tuesday\"\n",
        "    when day = 4 then \"Wednesday\"\n",
        "    when day = 5 then \"Thursday\"\n",
        "    when day = 6 then \"Friday\"\n",
        "    when day = 7 then \"Saturday\" end as day\n",
        "    FROM (\n",
        "      SELECT user_pseudo_id, day, ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY pages_viewed DESC) AS row_num\n",
        "      FROM (\n",
        "        SELECT a.user_pseudo_id, EXTRACT(DAYOFWEEK FROM PARSE_DATE('%Y%m%d',event_date)) AS day, sum(case when event_name = 'page_view' then 1 else 0 end) AS pages_viewed\n",
        "        FROM `{ga_project_id}.{ga_dataset}.{ga_data}` a\n",
        "        left join visitors_labeled b\n",
        "        on a.user_pseudo_id = b.user_pseudo_id\n",
        "        where {filters}\n",
        "        and (a.event_timestamp < IFNULL(first_transaction_session, 0)\n",
        "        or first_transaction_session is null)\n",
        "        GROUP BY user_pseudo_id, day ) )\n",
        "    WHERE row_num = 1 ),\n",
        "\n",
        "\n",
        "  engagement as (\n",
        "    select\n",
        "        user_pseudo_id,\n",
        "        safe_divide(sum(session_engaged),count(distinct session_id))  as engagement_rate,\n",
        "        round(sum(engagement_time_msec)/1000) as engagement_time_seconds,\n",
        "        safe_divide(sum(distinct case when session_engaged = 0 then 1 else 0 end),count(distinct session_id)) as bounce_rate,\n",
        "        count(distinct session_id) as total_sessions\n",
        "    from (\n",
        "        select\n",
        "            user_pseudo_id,\n",
        "            (select value.int_value from unnest(event_params) where key = 'ga_session_id') as session_id,\n",
        "            max((select value.int_value from unnest(event_params) where key = 'session_engaged')) as session_engaged,\n",
        "            max((select value.int_value from unnest(event_params) where key = 'engagement_time_msec')) as engagement_time_msec\n",
        "    from `{ga_project_id}.{ga_dataset}.{ga_data}` a\n",
        "    where {filters}\n",
        "        group by\n",
        "            user_pseudo_id,\n",
        "            session_id)\n",
        "    group by user_pseudo_id\n",
        "    ),\n",
        "\n",
        "  users_sessions as (\n",
        "  select z.*, b.dma as visited_dma,\n",
        "    c.daypart as visited_daypart,\n",
        "    d.day as visited_dow,\n",
        "    e.engagement_rate,\n",
        "    e.engagement_time_seconds,\n",
        "    e.total_sessions\n",
        "    from (\n",
        "    SELECT a.user_pseudo_id,\n",
        "    max(label) as label,\n",
        "    sum(case when event_name = 'page_view' then 1 else 0 end) as pageviews,\n",
        "    safe_divide(sum(case when event_name = 'page_view' then 1 else 0 end), count(distinct a.user_pseudo_id)) as avg_session_depth,\n",
        "    max(case when device.category = 'mobile' then 1 else 0 end) as mobile,\n",
        "    max(case when device.web_info.browser = 'Chrome' then 1 else 0 end) as Chrome,\n",
        "    max(case when device.web_info.browser = 'Safari' then 1 else 0 end) as Safari,\n",
        "    max(case when device.web_info.browser <> 'Chrome' and device.web_info.browser not like '%Safari%' then 1 else 0 end) as browser_other,\n",
        "    sum(case when traffic_source.medium = '(none)' then 1 else 0 end) as visits_traffic_source_none,\n",
        "    sum(case when traffic_source.medium = 'organic' then 1 else 0 end) as visits_traffic_source_organic,\n",
        "    sum(case when traffic_source.medium = 'cpc' then 1 else 0 end) as visits_traffic_source_cpc,\n",
        "    sum(case when traffic_source.medium = 'cpm' then 1 else 0 end) as visits_traffic_source_cpm,\n",
        "    sum(case when traffic_source.medium = 'affiliate' then 1 else 0 end) as visits_traffic_source_affiliate,\n",
        "    sum(case when traffic_source.medium = 'referral' then 1 else 0 end) as visits_traffic_source_referral,\n",
        "    count(distinct geo.metro) as distinct_dmas,\n",
        "    count(distinct EXTRACT(DAYOFWEEK FROM PARSE_DATE('%Y%m%d', event_date))) as num_diff_days_visited,\n",
        "    case when min(first_transaction_date) is null then DATE_DIFF(PARSE_DATE('%Y%m%d', CAST({end_date} as STRING)), min(PARSE_DATE('%Y%m%d',event_date)), DAY) else DATE_DIFF(min(PARSE_DATE('%Y%m%d',first_transaction_date)), min(PARSE_DATE('%Y%m%d',event_date)), DAY) end as days_since_first_visit,\n",
        "    case when min(first_transaction_date) is null then DATE_DIFF(PARSE_DATE('%Y%m%d', CAST({end_date} as STRING)), MAX(PARSE_DATE('%Y%m%d',event_date)), DAY) else DATE_DIFF(min(PARSE_DATE('%Y%m%d',first_transaction_date)), MAX(PARSE_DATE('%Y%m%d',event_date)), DAY) end as days_since_last_visit\n",
        "    from `{ga_project_id}.{ga_dataset}.{ga_data}` a\n",
        "    left join visitors_labeled b\n",
        "    on a.user_pseudo_id = b.user_pseudo_id\n",
        "    where {filters}\n",
        "    and (a.event_timestamp < IFNULL(first_transaction_session, 0)\n",
        "    or first_transaction_session is null)\n",
        "    group by a.user_pseudo_id) z\n",
        "    left join visitor_city b\n",
        "    on z.user_pseudo_id = b.user_pseudo_id\n",
        "    left join visitor_common_daypart c\n",
        "    on z.user_pseudo_id = c.user_pseudo_id\n",
        "    left join visitor_common_day d\n",
        "    on z.user_pseudo_id = d.user_pseudo_id\n",
        "    left join engagement e\n",
        "    on z.user_pseudo_id = e.user_pseudo_id)\n",
        "\n",
        "  select a.*, b.* except (user_pseudo_id), case when RAND() < 0.10 then 1 else 0 end as hold_out\n",
        "  from users_sessions a\n",
        "  left join (\n",
        "    {event_level_query}\n",
        "    left join visitors_labeled b\n",
        "    on a.user_pseudo_id = b.user_pseudo_id\n",
        "    where {filters}\n",
        "    and (a.event_timestamp < IFNULL(first_transaction_session, 0)\n",
        "    or first_transaction_session is null)\n",
        "    group by 1\n",
        "  ) b\n",
        "  on a.user_pseudo_id = b.user_pseudo_id\n",
        "  where a.engagement_rate > 0;\n",
        "  \"\"\"\n",
        "\n",
        "  # Define functions to get correlation matrix and plot distribution\n",
        "\n",
        "def corr_matrix(df, absolute=False, force_dtype=True, figsize=(20, 16)):\n",
        "  \"\"\"Plot a heatmap of a correlation matrix.\n",
        "\n",
        "  Keyword arguments:\n",
        "    df -- a dataframe\n",
        "    absolute -- Takes absolute values of correlation coefficient (default to False)\n",
        "    force_dtype -- Tries to transform all object dtypes into floats (default True)\n",
        "  \"\"\"\n",
        "\n",
        "  # Make a copy of the dataframe\n",
        "  df2 = df.copy()\n",
        "\n",
        "  # Transform data types to floats to prep for correlation matrix, since objects cannot be an input\n",
        "  if force_dtype:\n",
        "    for col, dtype in zip(df2.columns.values, df2.dtypes):\n",
        "      if dtype == 'object':\n",
        "        try:\n",
        "          df2[col] = df2[col].astype(float)\n",
        "        except ValueError:\n",
        "          try:\n",
        "            df2[col] = df2[col].str.rstrip('%').astype('float') / 100.0\n",
        "          except ValueError:\n",
        "            print(f\"Error transforming {col} into float! Removing from correlation matrix.\")\n",
        "            df2 = df2.drop(columns=col)\n",
        "\n",
        "\n",
        "  # Generate correlation matrix\n",
        "  if absolute:\n",
        "    corr = df2.corr().abs()\n",
        "  else:\n",
        "    corr = df2.corr()\n",
        "\n",
        "  # Generate a mask for the upper triangle\n",
        "  mask = np.zeros_like(corr, dtype=np.bool)\n",
        "  mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "  # Set up the matplotlib figure\n",
        "  f, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  # Generate a custom diverging colormap\n",
        "  cmap = sns.diverging_palette(10, 140, as_cmap=True)\n",
        "\n",
        "  # Draw the heatmap with the mask and correct aspect ratio\n",
        "  if absolute:\n",
        "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=0, center=0,\n",
        "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "  else:\n",
        "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
        "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "\n",
        "  plt.title(\"Correlation Matrix\", loc='center', fontsize='large', fontweight='bold')\n",
        "\n",
        "  return corr\n",
        "\n",
        "def plot_density(df, cols=None, scaled='normalize', title=\"Density Plot\", figsize=(8, 5),\n",
        "                 hist=True, xlabel=\"Distribution\", ylabel=\"Density\"):\n",
        "  \"\"\"Plots density of multiple variables and labels them.\n",
        "\n",
        "  Keyword arguments:\n",
        "    df -- input DataFrame\n",
        "    cols -- list of column names to plot (default None, will plot all data if possible)\n",
        "    scaled -- method of scaling data (default \"normalize\", can be set to None)\n",
        "    title -- title of chart\n",
        "    figsize -- pyplot figsize\n",
        "    hist -- Boolean, include histogram or not (default False)\n",
        "  \"\"\"\n",
        "  def _scale_data(df, cols=None, method=\"normalize\"):\n",
        "    # Make a copy of the dataframe\n",
        "    df_copy = df.copy(deep=True)\n",
        "\n",
        "    # loop through columns to normalize\n",
        "    if cols is None:\n",
        "      cols = df.columns.values\n",
        "\n",
        "    for col in cols:\n",
        "      if method == \"normalize\":\n",
        "        try:\n",
        "          df_copy[col] = (df_copy[col]-df_copy[col].min())/(df_copy[col].max()-df_copy[col].min())\n",
        "        except:\n",
        "          None\n",
        "      if method == \"standardize\":\n",
        "        try:\n",
        "          df_copy[col] = (df_copy[col]-df_copy[col].mean())/df_copy[col].std()\n",
        "        except:\n",
        "          None\n",
        "\n",
        "    return df_copy\n",
        "  # make a copy of dataframe\n",
        "  df_copy = df.copy(deep=True)\n",
        "\n",
        "  # Check dtypes for non-objects, transform as floats if possible\n",
        "  for field, dtype in zip(df_copy.columns.values, df_copy.dtypes):\n",
        "    if dtype == 'object':\n",
        "      try:\n",
        "        df_copy[field] = df_copy[field].astype(float)\n",
        "      except ValueError:\n",
        "        print(f\"Error transforming {field} into float! Removing from density plot.\")\n",
        "        df_copy = df_copy.drop(columns=field)\n",
        "\n",
        "  # Scale data based on input\n",
        "  if scaled == 'normalize':\n",
        "    df_copy = _scale_data(df_copy, cols=cols, method='normalize')\n",
        "  elif scaled == 'standardize':\n",
        "    df_copy = _scale_data(df_copy, cols=cols, method='standardize')\n",
        "  else:\n",
        "    xlabel = \"Values\"\n",
        "\n",
        "  # set figsize on figure object\n",
        "  plt.figure(figsize=figsize)\n",
        "\n",
        "  # Plot each field\n",
        "  if cols is None:\n",
        "    for field in df_copy.columns.values:\n",
        "      sns.distplot(df_copy[field], hist=hist, label=field,\n",
        "                   kde=True, kde_kws = {'linewidth': 2})\n",
        "  else:\n",
        "    for field in cols:\n",
        "      sns.distplot(df_copy[field], hist=hist, label=field,\n",
        "                   kde=True, kde_kws = {'linewidth': 2})\n",
        "\n",
        "\n",
        "  plt.legend(title=\"Legend\")\n",
        "  plt.title(title, loc='center', fontsize='large', fontweight='bold')\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  #plt.show()\n",
        "\n",
        "def compare_labels(df, col, label='label', user_col='num_users', figsize=(9,5),\n",
        "                   trim_max=None, trim_min=None):\n",
        "  \"\"\"\n",
        "  Sums 'user_col' by inputs 'label' and 'col', then plots the two labels against each other.\n",
        "  Defaults to using 'label' as the label column and 'num_users' as the count of users column.\n",
        "\n",
        "  Keyword arguments:\n",
        "    df -- input dataframe\n",
        "    col -- column in df you want to compare\n",
        "\n",
        "  Arguments:\n",
        "    label = 'label' -- name of label column; label of audiences you want to compare\n",
        "    user_col = 'num_users' -- column which should contain the count of users in your dataset\n",
        "    figsize = (9,5) -- figsize of resulting figures to plot\n",
        "  \"\"\"\n",
        "\n",
        "  df2 = df.copy(deep=True)\n",
        "  df2 = pd.DataFrame(df2.groupby([label, col])[user_col].sum())\n",
        "  df2.reset_index(inplace=True)\n",
        "\n",
        "  if trim_max is not None:\n",
        "    df2[col] = df2[col].astype(float)\n",
        "    df2 = df2.loc[df2[col] <= float(trim_max)]\n",
        "\n",
        "  if trim_min is not None:\n",
        "    df2[col] = df2[col].astype(float)\n",
        "    df2 = df2.loc[df2[col] >= float(trim_max)]\n",
        "\n",
        "  fig, ax1 = plt.subplots(figsize=figsize)\n",
        "  ax2 = ax1.twinx()\n",
        "\n",
        "  ln1 = ax1.plot(np.array(df2.loc[df2[label] == 1, col]), np.array(df2.loc[df2[label] == 1, user_col]),\n",
        "    color='r', label='1')\n",
        "\n",
        "  ln2 = ax2.plot(np.array(df2.loc[df2[label] == 0, col]), np.array(df2.loc[df2[label] == 0, user_col]),\n",
        "    color='b', label='0')\n",
        "\n",
        "  lns = ln1+ln2\n",
        "  labs = [l.get_label() for l in lns]\n",
        "  ax1.legend(lns, labs, loc=0)\n",
        "\n",
        "  plt.title(col)\n",
        "\n",
        "# Helper function to look at distributions of data\n",
        "def query_and_compare(col, trim_max=None, trim_min=None, figsize=None):\n",
        "  dc['col'] = col\n",
        "  query = f\"\"\"\n",
        "  select label, {col}, count(*) as num_users\n",
        "  from `{destination_project_id}.{destination_dataset}.{destination_table}`\n",
        "  where engagement_rate > 0\n",
        "  group by 1, 2\n",
        "  \"\"\"\n",
        "\n",
        "  df = query_to_df(query, destination_project_id)\n",
        "\n",
        "  compare_labels(df, col=col, figsize=figsize, trim_max=trim_max,\n",
        "                 trim_min=trim_min)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4pycImup7v4"
      },
      "source": [
        "### 1.2) Set Parameters to Pull in GA4 Export Data from BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfVvTKUSnB0U",
        "cellView": "form"
      },
      "source": [
        "# Fill in query / feature\n",
        "\n",
        "ga_project_id = \"\" #@param {type:\"string\"}\n",
        "ga_dataset = \"\" #@param {type:\"string\"}\n",
        "ga_data = \"\" #@param {type:\"string\"}\n",
        "start_date = \"\" #@param {type:\"string\"}\n",
        "end_date = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK7BBdX6cY1A"
      },
      "source": [
        "### 1.3) Filtering GA Export Data Based on Dates Above\n",
        "\n",
        "*   [Optional]: Add additional filters based on the business use case\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prKkfPvDnB5P",
        "cellView": "form"
      },
      "source": [
        "# @title\n",
        "filters = f\"\"\"\n",
        "  _TABLE_SUFFIX BETWEEN '{start_date}' AND '{end_date}'\n",
        "  AND geo.Country=\"United States\"\n",
        "  \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#value_query = f\"\"\"\n",
        "#  SELECT event_name, SUM(rev)/COUNT(user_pseudo_id) AS value FROM\n",
        "#  (SELECT a.event_name, a.user_pseudo_id, AVG(b.rev)/COUNT(a.event_name) AS rev FROM (select * from `{ga_project_id}.{ga_dataset}.{ga_data}` WHERE {filters}) a INNER JOIN\n",
        "#  (SELECT user_pseudo_id, SUM(ecommerce.purchase_revenue) AS rev FROM `{ga_project_id}.{ga_dataset}.{ga_data}` WHERE {filters} GROUP BY user_pseudo_id) b\n",
        "#  ON a.user_pseudo_id = b.user_pseudo_id GROUP BY a.event_name, a.user_pseudo_id) GROUP BY event_name\n",
        "#\"\"\"\n",
        "\n",
        "conversion_value_query = f\"\"\"\n",
        "  select event_name,  COUNT(event_name), SUM(ecommerce.purchase_revenue) from `{ga_project_id}.{ga_dataset}.{ga_data}` WHERE {filters} GROUP BY event_name\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "T0MyYqVWpZDp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4) Define Target Variable and Use Case\n",
        "\n",
        "**Instructions:** Run block of code, select \"Conversion Action\" and \"Bid Towards\", then move to next block of code. Do no re-run this block.\n",
        "\n",
        "**Use Case #1**: Online conversions  and revenue available\n",
        "\n",
        "*   Conversion Action: Select conversion event\n",
        "*   Bid Towards: Revenue\n",
        "\n",
        "**Use Case #2**: Non revenue conversion event (i.e. lead form, request additional info, etc.)\n",
        "\n",
        "*   Conversion Action: Select conversion event\n",
        "*   Bid Towards: Low Funnel Activity\n",
        "\n",
        "**Use Case #3**: Deeper business goal (i.e. >2 purchases in 6 months, conversions > $500, etc.)\n",
        "\n",
        "*   Conversion Action: Select conversion event\n",
        "*   Bid Towards: Deeper Business Goal\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EkHGNMhEFsIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "conversion_value_mapping = query_to_df(conversion_value_query, ga_project_id)\n",
        "conversion_value_mapping.set_index('event_name', inplace=True)\n",
        "conversion_dropdown = widgets.Dropdown(\n",
        "    options=conversion_value_mapping.index.tolist(),\n",
        "    description='Conversion Action:',\n",
        "    disabled=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "bid_towards_dropdown = widgets.Dropdown(\n",
        "    options=['Revenue', 'Low Funnel Activity', 'Deeper Business Goal'],\n",
        "    description='Bid Towards:',\n",
        "    disabled=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "widgets.HBox([conversion_dropdown, bid_towards_dropdown])"
      ],
      "metadata": {
        "id": "QCjqa36kOb9f",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "conversion_event = conversion_dropdown.value\n",
        "def build_top_k_action_query(actions, is_only_converters):\n",
        "  actions_query = ''\n",
        "  for i in actions:\n",
        "    actions_query += \"'\" + i + \"' in UNNEST(a) and \"\n",
        "  actions_query = actions_query[:-4]\n",
        "  converter_proportion_top_k = f\"\"\"\n",
        "    select count(distinct user_pseudo_id) from `{ga_project_id}.{ga_dataset}.{ga_data}` WHERE {filters} and user_pseudo_id in\n",
        "      (select user_pseudo_id from\n",
        "        (select user_pseudo_id, ARRAY_AGG(event_name) as a FROM `{ga_project_id}.{ga_dataset}.{ga_data}` WHERE {filters} group by user_pseudo_id)\n",
        "      where\n",
        "      {actions_query}\n",
        "    ) {'and event_name = \"' + conversion_event + '\"' if is_only_converters else ''}\n",
        "  \"\"\"\n",
        "  return converter_proportion_top_k"
      ],
      "metadata": {
        "id": "Y7gv7IY8M0zH",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5) Initial EDA at the Event Level\n",
        "\n",
        "Learn more about event type [here](https://support.google.com/analytics/answer/9322688?hl=en#zippy=%2Crealtime-report%2Cdebugview-report)."
      ],
      "metadata": {
        "id": "ytEpPKumDf8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "label_query = f\"\"\"\n",
        "  visitors_labeled as (\n",
        "  select user_pseudo_id,\n",
        "  min(case when event_name = '{conversion_event}' then event_timestamp end) as first_transaction_session,\n",
        "  min(case when event_name = '{conversion_event}' then event_date end) as first_transaction_date,\n",
        "  max(case when event_name = '{conversion_event}' then 1 else 0 end) as label\n",
        "  from `{ga_project_id}.{ga_dataset}.{ga_data}` a\n",
        "  where {filters}\n",
        "  group by user_pseudo_id\n",
        "  )\"\"\"\n",
        "query_to_df(build_eda_query(label_query, ga_project_id, ga_dataset, ga_data, filters), ga_project_id)"
      ],
      "metadata": {
        "id": "_mzi_tSbEH5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.6) Create event level dataset"
      ],
      "metadata": {
        "id": "0RRw9yLqGGKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#  [Optional] add additional features manually\n",
        "#  Adding additional unnests will cause duplicates without additional edits\n",
        "#  to the code.\n",
        "\n",
        "#  event_level_query = f\"\"\"select a.user_pseudo_id,\n",
        "#  max(case when event_name = 'add_payment_info' then 1 else 0 end) as campus_collection_user_flag,\n",
        "#  max(case when event_name = 'begin_checkout' then 1 else 0 end) as view_item_list_flag,\n",
        "#  max(case when event_name = 'add_to_cart' then 1 else 0 end) as san_francisco_users_flag,\n",
        "#  max(case when event_name = 'select_item' then 1 else 0 end) as android_lovers_flag,\n",
        "#  max(case when event_name = 'view_item' then 1 else 0 end) as select_promotion_flag,\n",
        "#  max(case when event_name = 'select_promotion' then 1 else 0 end) as view_promotion_flag\n",
        "\n",
        "#  from `{ga_project_id}.{ga_dataset}.{ga_data}` a\"\"\"\n",
        "\n",
        "event_level_query = f\"\"\"select a.user_pseudo_id,\n",
        "\"\"\"\n",
        "for event in conversion_value_mapping.index.tolist():\n",
        "  if event != '404_error' and event != conversion_event:\n",
        "    event_col_name = re.sub(r'[ -]', '_', event)\n",
        "    event_col_name = re.sub(r'[^a-zA-Z0-9_]', '', event_col_name)\n",
        "    event_level_query += f\"\"\"\n",
        "      max(case when event_name = '{event}' then 1 else 0 end) as {event_col_name},\"\"\"\n",
        "event_level_query += f\"\"\"\n",
        "  from `{ga_project_id}.{ga_dataset}.{ga_data}` a\"\"\""
      ],
      "metadata": {
        "id": "3Ja2lXt0E1hI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.7) Save final dataset to BigQuery\n",
        "Dataset and Table names must only include letters, numbers, and underscores\n"
      ],
      "metadata": {
        "id": "97gcjMDnDYLC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h6NkB0kg0cS",
        "cellView": "form"
      },
      "source": [
        "destination_project_id = \"\" #@param {type:\"string\"}\n",
        "destination_dataset = \"\" #@param {type:\"string\"}\n",
        "destination_table = \"\" #@param {type:\"string\"}\n",
        "\n",
        "client = bigquery.Client(destination_project_id)\n",
        "\n",
        "try:\n",
        "  dataset = bigquery.Dataset(destination_project_id + '.' +destination_dataset)\n",
        "  dataset.location = \"US\"\n",
        "  dataset = client.create_dataset(dataset, timeout=30)\n",
        "  print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
        "except:\n",
        "  print('Dataset already exists, skipping creation...')\n",
        "\n",
        "\n",
        "job_config = bigquery.QueryJobConfig()\n",
        "table_ref = client.dataset(destination_dataset).table(destination_table)\n",
        "job_config.destination = table_ref\n",
        "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "\n",
        "# Start the query, passing in the extra configuration.\n",
        "query_job = client.query(\n",
        "    create_build_feature_set_sql(label_query, ga_project_id, ga_dataset, ga_data, filters, end_date, event_level_query),\n",
        "    location=\"US\",\n",
        "    job_config=job_config\n",
        ")\n",
        "\n",
        "query_job.result()\n",
        "print(f\"Query results loaded to table {table_ref.path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrFfQsMNWo8d"
      },
      "source": [
        "# 2) Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1)  Check Distribution of Labels\n",
        "\n",
        "Conversion data is often **imbalanced** (significantly more non converters than converters). We will use the ratio to help balance the dataset for model training.\n",
        "\n",
        "For more info click [here](https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data)."
      ],
      "metadata": {
        "id": "yyYrTTPdEJyJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rrh6dpVFjCB",
        "cellView": "form"
      },
      "source": [
        "# @title\n",
        "query = f\"select label, count(*) as users from `{destination_project_id}.{destination_dataset}.{destination_table}` group by 1;\"\n",
        "dist = query_to_df(query, destination_project_id)\n",
        "print(dist)\n",
        "print(\"\\nRatio of 0's to 1's: \")\n",
        "dist.set_index('label', inplace=True)\n",
        "print(dist.at[0,'users']/dist.at[1,'users'])\n",
        "sample_rate = int(dist.at[0,'users']/dist.at[1,'users'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2) Correlation Matrix"
      ],
      "metadata": {
        "id": "viUbmAlWEYzj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJhHKwduv_AO",
        "cellView": "form"
      },
      "source": [
        "# @title\n",
        "query = f\"select label, count(*) as users from `{destination_project_id}.{destination_dataset}.{destination_table}` group by 1;\"\n",
        "users_to_sample = 50000\n",
        "sampler = float(1/(dist.sum()/users_to_sample))\n",
        "dist = query_to_df(query, destination_project_id)\n",
        "dist.set_index('label', inplace=True)\n",
        "dist.at[0,'users']/dist.at[1,'users']\n",
        "sampled_data = query_to_df(f'select * from `{destination_project_id}.{destination_dataset}.{destination_table}` where RAND() < {sampler}', destination_project_id)\n",
        "corr = corr_matrix(sampled_data, figsize=(12,9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3) [Optional]: Enter your email address to receive correlation matrix via Google Sheets"
      ],
      "metadata": {
        "id": "E6ehOcRbE9S-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED5eRzeGh9fv",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "email = \"\" #@param {type:\"string\"}\n",
        "\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "#sh = gc.open_by_url(sheet_url)\n",
        "#corr = corr_matrix(sampled_data, figsize=(12,9))\n",
        "\n",
        "sh = gc.create('Correlation Matrix')\n",
        "worksheet = sh.add_worksheet(title=\"Corr\", rows=\"1000\", cols=\"100\")\n",
        "sh.del_worksheet(sh.sheet1)\n",
        "\n",
        "def num_to_col_letters(num):\n",
        "    letters = ''\n",
        "    while num:\n",
        "        mod = (num-1) % 26\n",
        "        letters += chr(mod + 65)\n",
        "        num = (num-1) // 26\n",
        "    return ''.join(reversed(letters))\n",
        "\n",
        "def df_to_sheet(df, sh):\n",
        "\n",
        "  ran = 'A1:'+num_to_col_letters(df.shape[1])+str(df.shape[0]+1)\n",
        "  cell_list = worksheet.range(ran)\n",
        "  df_list = df.values.tolist()\n",
        "  flat_list = [item for sublist in df_list for item in sublist]\n",
        "  list_headers = list(df.columns.values)\n",
        "  flat_list=list_headers+flat_list\n",
        "\n",
        "  for cell, value in zip(cell_list, flat_list):\n",
        "    if value == value:\n",
        "      cell.value = value\n",
        "    else:\n",
        "      cell.value = 0\n",
        "  worksheet.update_cells(cell_list)\n",
        "\n",
        "rule = ConditionalFormatRule(\n",
        "    ranges=[GridRange.from_a1_range('A1:'+num_to_col_letters(corr.shape[1])+str(corr.shape[0]+1), worksheet)],\n",
        "    gradientRule=GradientRule(\n",
        "        minpoint=InterpolationPoint(color=Color(204/255,0,0), value = '-1', type='NUMBER'),\n",
        "        midpoint=InterpolationPoint(color=Color(1,1,1), value = '0', type='NUMBER'),\n",
        "        maxpoint=InterpolationPoint(color=Color(56/255,118/255,29/255), value = '1', type='NUMBER')\n",
        "    )\n",
        ")\n",
        "\n",
        "rules = get_conditional_format_rules(worksheet)\n",
        "rules.append(rule)\n",
        "rules.save()\n",
        "\n",
        "df_to_sheet(corr, worksheet)\n",
        "\n",
        "worksheet.insert_cols([[None] + list(corr.columns.values)])\n",
        "\n",
        "sh.share(email, perm_type='user', role='writer')\n",
        "print(\"Correlation matrix has been shared to: {}\".format(email))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI-I9XqMsNyZ"
      },
      "source": [
        "# 3) Model Training & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1) Remove Fields Not Needed in Final Model\n",
        "\n",
        "*   Seperate fields with a comma\n",
        "* Remove  \"user_pseudo_id\"\n",
        "* Remove any variable that is perfectly correlated with your target variable (i.e. target var is purchase, you would want to remove a \"Thank you\" page visit)\n",
        "\n"
      ],
      "metadata": {
        "id": "P5kD68zlFG4j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IjrTq9lqVsW",
        "cellView": "form"
      },
      "source": [
        "remove_fields = \"hold_out, user_pseudo_id, days_since_last_visit, days_since_first_visit\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2) Create Many Iterations of Models\n",
        "\n",
        "Models tested:\n",
        "\n",
        "*   Logistic Regression\n",
        "*   XG Boost\n",
        "\n",
        "L1 & L2 regularization tested: 0, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3\n",
        "\n",
        "FYI: for more information about creating a model in BigQuery, click [here](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LvId36VXFUCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_mode = True #@param {type:\"boolean\"}"
      ],
      "metadata": {
        "id": "i_Iijb_uXTWG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL-9GSDJYPNU",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# Start with empty dict and empy list of model names\n",
        "# By default, only includes logit regression and non-reduced feature list\n",
        "import math\n",
        "model_dict = {}\n",
        "model_name_list = []\n",
        "model_type_list = ['LOGISTIC_REG', 'BOOSTED_TREE_CLASSIFIER']\n",
        "Enable_Global_Explainability = True\n",
        "model_name_base = \"ga4_propensity_model_\"\n",
        "model_project = destination_project_id\n",
        "model_dataset = destination_dataset\n",
        "label_1_weight = 1.0\n",
        "label_0_weight = round(math.sqrt(sample_rate/2.0))\n",
        "l1_reg = ['0', '0.0001', '0.001', '0.01', '0.1', '0.2', '0.3'] if not test_mode else ['0']\n",
        "l2_reg = ['0', '0.0001', '0.001', '0.01', '0.1', '0.2', '0.3'] if not test_mode else ['0']\n",
        "\n",
        "standard_features = f\"\"\"* except ({remove_fields})\"\"\"\n",
        "\n",
        "for model in client.list_models(destination_dataset):\n",
        "  client.delete_model(f'{destination_project_id}.{destination_dataset}.{model.model_id}')\n",
        "\n",
        "def create_model(model, l1_reg_input, l2_reg_input):\n",
        "  global label_1_weight\n",
        "  global label_0_weight\n",
        "  global l1_reg\n",
        "  global l2_reg\n",
        "  global model_project\n",
        "  global model_dataset\n",
        "  global model_name_base\n",
        "  global sample_rate\n",
        "  global Include_XGBoost\n",
        "  global Enable_Global_Explainability\n",
        "  global remove_fields\n",
        "\n",
        "  CLASS_WEIGHTS = f\"\"\",CLASS_WEIGHTS = [('1', {label_1_weight}), ('0', {label_0_weight})]\"\"\"\n",
        "  OPTIMIZE_STRATEGY = ''\n",
        "\n",
        "  model_type = model\n",
        "  model_prefix = model[0:5]\n",
        "\n",
        "\n",
        "  model_name = model_name_base + '_' + model_prefix + '_l1_' + str(l1_reg_input).replace(\".\",\"_\") + '_l2_' + str(l2_reg_input).replace(\".\",\"_\")\n",
        "  model_name_list.append(model_name)\n",
        "  model_dict[model_name] = {}\n",
        "  model_dict[model_name]['model_type'] = model_type\n",
        "  client = bigquery.Client(destination_project_id)\n",
        "\n",
        "  job_config = bigquery.QueryJobConfig()\n",
        "\n",
        "  model_query = f\"\"\"CREATE MODEL\n",
        "    `{model_project}.{model_dataset}.{model_name}` OPTIONS\n",
        "    (model_type='{model_type}',\n",
        "    labels = ['label'],\n",
        "    L1_REG = {l1_reg_input},\n",
        "    L2_REG = {l2_reg_input},\n",
        "    DATA_SPLIT_METHOD = 'RANDOM',\n",
        "    DATA_SPLIT_EVAL_FRACTION = 0.20\n",
        "    {CLASS_WEIGHTS}\n",
        "    {OPTIMIZE_STRATEGY},\n",
        "    ENABLE_GLOBAL_EXPLAIN = {Enable_Global_Explainability}) AS\n",
        "    (select * except ({remove_fields}) from `{destination_project_id}.{destination_dataset}.{destination_table}`\n",
        "    where label = 1 and hold_out = 0\n",
        "    union all\n",
        "    select * except ({remove_fields}) from `{destination_project_id}.{destination_dataset}.{destination_table}`\n",
        "    where label = 0 and hold_out = 0\n",
        "    and mod(abs(Farm_Fingerprint (user_pseudo_id)),{round(math.sqrt(sample_rate/2))})=0)\"\"\"\n",
        "\n",
        "  query_job = client.query(\n",
        "    model_query,\n",
        "    location=\"US\",\n",
        "    job_config=job_config,\n",
        "  )\n",
        "\n",
        "  query_job.result()\n",
        "  #print(\"Model training completed\")\n",
        "\n",
        "# Iterate through the model types, and different regularization levels\n",
        "progress_bar_started = widgets.IntProgress(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=98,\n",
        "    description='Model Training Started:',\n",
        "    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    style={'bar_color': 'white', 'description_width': 'initial'},\n",
        "    orientation='horizontal'\n",
        ")\n",
        "display(progress_bar_started)\n",
        "procs = []\n",
        "for model in model_type_list:\n",
        "  for reg1 in l1_reg:\n",
        "    for reg2 in l2_reg:\n",
        "      proc = Process(target=create_model, args=(model, reg1, reg2))\n",
        "      procs.append(proc)\n",
        "      proc.start()\n",
        "      progress_bar_started.value += 1\n",
        "\n",
        "progress_bar_completed = widgets.IntProgress(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=98,\n",
        "    description='Model Training Finished:',\n",
        "    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    style={'bar_color': 'green', 'description_width': 'initial'},\n",
        "    orientation='horizontal'\n",
        ")\n",
        "display(progress_bar_completed)\n",
        "\n",
        "for proc in procs:\n",
        "  proc.join()\n",
        "  progress_bar_completed.value += 1\n",
        "print(\"Model Types Tested: \" + str(model_type_list))\n",
        "print(\"L1 Reg Tested: \" + str(l1_reg))\n",
        "print(\"L2 Reg Tested: \" + str(l2_reg))\n",
        "print(\"All training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3) Select Model Performance Metric\n",
        "\n",
        "**Instructions:** Run block of code, select \"Evaluation Metric\", then move to next block of code. Do no re-run this block.\n",
        "\n",
        "This criteria will be used to identify the best fit model.\n",
        "\n",
        "\n",
        "*   F1 & ROC AUC are best suited for imbalanced data\n",
        "\n",
        "**Note:** **Before proceeding,** if you would like to view additional details about each model created, like full evaluation metrics & feature importance, you can check them out individually in the BigQuery Console.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kTUKQ-Krlzcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "evaluation_metric = widgets.Dropdown(\n",
        "    options=[('Accuracy', 'accuracy'), ('Precision', 'precision'), ('Recall', 'recall'), ('F1 Score', 'f1Score'), ('ROC AUC', 'rocAuc')],\n",
        "    value='accuracy',\n",
        "    description='Evaluation Metric:',\n",
        "    disabled=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "evaluation_metric"
      ],
      "metadata": {
        "id": "VknSE5GAbI58",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4) Select Best Model\n",
        "\n",
        "We identify the model with the highest performance on the **test data**, according to the criteria selected above."
      ],
      "metadata": {
        "id": "t0GiwHoXl1od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "max = 0\n",
        "best_model = ''\n",
        "for model in client.list_models(destination_dataset):\n",
        "  resp = client.get_model(f'{destination_project_id}.{destination_dataset}.{model.model_id}')\n",
        "  #pp.pprint(resp.__dict__)\n",
        "  if 'binaryClassificationMetrics' in resp._properties['trainingRuns'][0]['evaluationMetrics'].keys():\n",
        "    if resp._properties['trainingRuns'][0]['evaluationMetrics']['binaryClassificationMetrics']['aggregateClassificationMetrics'][evaluation_metric.value] > max:\n",
        "      max = resp._properties['trainingRuns'][0]['evaluationMetrics']['binaryClassificationMetrics']['aggregateClassificationMetrics'][evaluation_metric.value]\n",
        "      best_model = model.model_id\n",
        "\n",
        "print(f\"Best model: {best_model}\")\n",
        "print(f\"{evaluation_metric.value.capitalize()} on holdout sample: {max}\")\n",
        "\n",
        "for model in client.list_models(destination_dataset):\n",
        "  if best_model != model.model_id :\n",
        "    client.delete_model(f'{destination_project_id}.{destination_dataset}.{model.model_id}')\n"
      ],
      "metadata": {
        "id": "jik59OxUO8or",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5) Calculate Global Explainability\n",
        "\n",
        "To learn more about Global Explainability click [here](https://cloud.google.com/bigquery/docs/xai-overview)."
      ],
      "metadata": {
        "id": "s77fh5bFl5kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "query=f\"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.GLOBAL_EXPLAIN(MODEL `{model_project}.{model_dataset}.{best_model}`)\n",
        "\"\"\"\n",
        "#Remove limit statement if all data is needed for deeper analysis\n",
        "explain=pd.read_gbq(query, project_id=model_project, dialect='standard')\n",
        "explain"
      ],
      "metadata": {
        "id": "pNU2CM_ZOsL3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Generate Custom Bidding Algorithm"
      ],
      "metadata": {
        "id": "f69H46IEWaXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1) Remove Fields Not Needed in Final Script\n",
        "\n",
        "For instance:\n",
        "*   Fields that are not biddable\n",
        "*   Fields with low attribution\n",
        "\n",
        "Seperate events with a comma.\n",
        "\n"
      ],
      "metadata": {
        "id": "wpyrstY0l77O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ignore_events = \"\" #@param {type:\"string\"}\n",
        "propertyId=0 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "Ba09Yxy3HXrB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2) Generate Custom Bidding Script!"
      ],
      "metadata": {
        "id": "8jnf3YSgXcey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can copy and paste this script directly into DV360.\n",
        "\n",
        "If you'd like to have the custom script pushed directly into DV360 from Colab, please continue to the next section."
      ],
      "metadata": {
        "id": "NGg2Uqv9Xsd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# This constant is defined because we want the sum of the HVAs to be 0.8\n",
        "sum_HVA_max_coeff = 1.25\n",
        "explain['attribution_scaled'] = explain['attribution'] / sum(explain['attribution'])\n",
        "explain['attribution_scaled_no_value'] = explain['attribution'] / (sum(explain['attribution']) * sum_HVA_max_coeff)\n",
        "\n",
        "script_end = '''\n",
        "  ])\n",
        "'''\n",
        "biddable_attributes = filter(lambda a : a in conversion_value_mapping.index.tolist() and a != conversion_event and a not in [s.strip() for s in ignore_events.split(',')], explain['feature'])\n",
        "biddable_attributes = list(biddable_attributes)\n",
        "converters = query_to_df(build_top_k_action_query(actions = [biddable_attributes[0], biddable_attributes[1], biddable_attributes[2]], is_only_converters = True), ga_project_id)['f0_'][0]\n",
        "all_actions = query_to_df(build_top_k_action_query(actions = [biddable_attributes[0], biddable_attributes[1], biddable_attributes[2]], is_only_converters = False), ga_project_id)['f0_'][0]\n",
        "\n",
        "avg_conversion_value = (conversion_value_mapping.loc[conversion_event]['f1_'] / conversion_value_mapping.loc[conversion_event]['f0_']) * (converters / all_actions)\n",
        "\n",
        "script_contents = 'sum_aggregate(['\n",
        "no_value_flag = False\n",
        "attribution_coeff_column = 'attribution_scaled'\n",
        "\n",
        "if bid_towards_dropdown.value == 'Low Funnel Activity':\n",
        "  attribution_coeff_column = 'attribution_scaled_no_value'\n",
        "  attribution_entry = explain.loc[explain['feature'] == conversion_event][attribution_coeff_column]\n",
        "  a_condition = 'sum_aggregate([([has_ga4_conversions('+str(propertyId)+', \"'+conversion_event+'\")], 1)])'\n",
        "elif bid_towards_dropdown.value == 'Deeper Business Goal':\n",
        "  a_condition = '0'\n",
        "else:\n",
        "  a_condition = 'ga4_conversions_total_value('+str(propertyId)+', \"'+conversion_event+'\")'\n",
        "\n",
        "for index, row in explain.iterrows():\n",
        "  if row['feature'] in conversion_value_mapping.index.tolist() and row['feature'] != conversion_event and row['feature'] not in ignore_events.split(', '):\n",
        "    value = row[attribution_coeff_column]\n",
        "    if  bid_towards_dropdown.value == 'Revenue':\n",
        "      value = row[attribution_coeff_column] * avg_conversion_value\n",
        "    script_contents += f'''\n",
        "        ([has_ga4_conversions({propertyId}, '{row['feature']}')], {value}),'''\n",
        "b_condition = script_contents + script_end\n",
        "\n",
        "conditional_script = f'''\n",
        "_a = {a_condition}\n",
        "_b = {b_condition}\n",
        "if _a > 0:\n",
        "  return _a\n",
        "else:\n",
        "  return _b\n",
        "'''\n",
        "print(conditional_script)\n",
        "\n",
        "with open('/content/script.txt', 'w') as writefile:\n",
        "    writefile.write(conditional_script)"
      ],
      "metadata": {
        "id": "v_6th5-FGnFW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3) [Optional]: Upload Script Directly to DV360"
      ],
      "metadata": {
        "id": "zvPvFOwDl-yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input credentials"
      ],
      "metadata": {
        "id": "2gOyvyj_YF3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from oauthlib.oauth2.rfc6749.errors import InvalidGrantError\n",
        "from urllib import parse\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "credentials = None\n",
        "\n",
        "client_id = \"\" #@param {type:\"string\"}\n",
        "client_secret = \"\" #@param {type:\"string\"}\n",
        "partnerId = 0 #@param {type:\"integer\"}\n",
        "advertiserId = 0 #@param {type:\"integer\"}\n",
        "SCOPES = ['https://www.googleapis.com/auth/adwords',\n",
        "           'https://www.googleapis.com/auth/dfareporting',\n",
        "           'https://www.googleapis.com/auth/dfatrafficking',\n",
        "           'https://www.googleapis.com/auth/ddmconversions',\n",
        "           \"https://www.googleapis.com/auth/analytics.edit\",\n",
        "           'https://www.googleapis.com/auth/spreadsheets.readonly',\n",
        "           'https://www.googleapis.com/auth/gmail.send',\n",
        "           'https://www.googleapis.com/auth/cloud-platform',\n",
        "           'https://www.googleapis.com/auth/display-video'\n",
        "           ]\n",
        "\n",
        "class ClientConfigBuilder(object):\n",
        "      \"\"\"Helper class used to build a client config dict used in the OAuth 2.0 flow.\n",
        "      \"\"\"\n",
        "      _DEFAULT_AUTH_URI = 'https://accounts.google.com/o/oauth2/auth'\n",
        "      _DEFAULT_TOKEN_URI = 'https://accounts.google.com/o/oauth2/token'\n",
        "      CLIENT_TYPE_WEB = 'web'\n",
        "      CLIENT_TYPE_INSTALLED_APP = 'installed'\n",
        "\n",
        "      def __init__(self, client_type=None, client_id=None, client_secret=None,\n",
        "                   auth_uri=_DEFAULT_AUTH_URI, token_uri=_DEFAULT_TOKEN_URI):\n",
        "          self.client_type = client_type\n",
        "          self.client_id = client_id\n",
        "          self.client_secret = client_secret\n",
        "          self.auth_uri = auth_uri\n",
        "          self.token_uri = token_uri\n",
        "\n",
        "      def Build(self):\n",
        "          \"\"\"Builds a client config dictionary used in the OAuth 2.0 flow.\"\"\"\n",
        "          if all((self.client_type, self.client_id, self.client_secret,\n",
        "                  self.auth_uri, self.token_uri)):\n",
        "              client_config = {\n",
        "                  self.client_type: {\n",
        "                      'client_id': self.client_id,\n",
        "                      'client_secret': self.client_secret,\n",
        "                      'auth_uri': self.auth_uri,\n",
        "                      'token_uri': self.token_uri\n",
        "                  }\n",
        "              }\n",
        "          else:\n",
        "              raise ValueError('Required field is missing.')\n",
        "\n",
        "          return client_config\n",
        "\n",
        "\n",
        "client_config = ClientConfigBuilder(\n",
        "    client_type=ClientConfigBuilder.CLIENT_TYPE_WEB, client_id=client_id,\n",
        "    client_secret=client_secret)\n",
        "\n",
        "flow = InstalledAppFlow.from_client_config(\n",
        "     client_config.Build(), scopes=SCOPES)\n",
        "# Note that from_client_config will not produce a flow with the\n",
        "# redirect_uris (if any) set in the client_config. This must be set\n",
        "# separately.\n",
        "flow.redirect_uri = 'http://localhost:8080'\n",
        "\n",
        "auth_url, _ = flow.authorization_url(prompt='consent')\n",
        "\n",
        "print('Log into the Google Account you use to access your AdWords account '\n",
        "      'and go to the following URL: \\n%s\\n' % auth_url)\n",
        "print('After approving the token copy and paste the full URL.')\n",
        "url = input('URL: ').strip()\n",
        "code = parse.parse_qs(parse.urlparse(url).query)['code'][0]\n",
        "\n",
        "try:\n",
        "    flow.fetch_token(code=code)\n",
        "except InvalidGrantError as ex:\n",
        "    print('Authentication has failed: %s' % ex)\n",
        "    sys.exit(1)\n",
        "\n",
        "token = flow.credentials.token\n",
        "refresh_token = flow.credentials.refresh_token"
      ],
      "metadata": {
        "id": "N5JpYAPjRrcN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credentials Check"
      ],
      "metadata": {
        "id": "UU0599aPmBTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from googleapiclient import discovery\n",
        "from apiclient.http import MediaFileUpload, HttpRequest\n",
        "from google.oauth2.credentials import Credentials\n",
        "credentials = None\n",
        "\n",
        "def get_credentials(token, refresh_token, client_id, client_secret):\n",
        "    global credentials\n",
        "    if credentials == None or credentials.expired:\n",
        "      credentials = Credentials(\n",
        "        token=token,\n",
        "        refresh_token=refresh_token,\n",
        "        client_id=client_id,\n",
        "        client_secret=client_secret,\n",
        "        token_uri=ClientConfigBuilder._DEFAULT_TOKEN_URI,\n",
        "        scopes=SCOPES)\n",
        "    return credentials\n",
        "\n",
        "# Build the discovery document URL.\n",
        "discovery_url = f'https://displayvideo.googleapis.com/$discovery/rest?version=v2'\n",
        "\n",
        "# Build the API service.\n",
        "service = discovery.build(\n",
        "    'displayvideo',\n",
        "    'v2',\n",
        "    discoveryServiceUrl=discovery_url,\n",
        "    credentials=get_credentials(token, refresh_token, client_id, client_secret))\n",
        "\n",
        "print(service.advertisers().list(partnerId=partnerId).execute())\n",
        "\n"
      ],
      "metadata": {
        "id": "WrXPuJ-6bmVC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload Script to DV360"
      ],
      "metadata": {
        "id": "o16VM98vmDea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "# Create a custom bidding algorithm object.\n",
        "custom_bidding_algorithm_obj = {\n",
        "    'advertiserId': advertiserId,\n",
        "    'displayName': 'GAP-autogenerated-custom-bidding-script',\n",
        "    'entityStatus': 'ENTITY_STATUS_ACTIVE',\n",
        "    'customBiddingAlgorithmType': 'SCRIPT_BASED'\n",
        "}\n",
        "\n",
        "# Create the custom bidding algorithm.\n",
        "response = service.customBiddingAlgorithms().create(\n",
        "    body=custom_bidding_algorithm_obj\n",
        ").execute()\n",
        "\n",
        "# Display the new custom bidding algorithm.\n",
        "print(f'The following Custom Bidding Algorithm was created: {response}')\n",
        "\n",
        "# Retrieve a usable custom bidding script reference\n",
        "# object.\n",
        "algorithmId = response['customBiddingAlgorithmId']\n",
        "custom_bidding_script_ref = service.customBiddingAlgorithms().uploadScript(\n",
        "    customBiddingAlgorithmId=algorithmId,\n",
        "    advertiserId=advertiserId\n",
        ").execute()\n",
        "\n",
        "# Display the new custom bidding script reference object.\n",
        "print('The following custom bidding script reference object was retrieved:'\n",
        "      f'{custom_bidding_script_ref}')\n",
        "\n",
        "# Create a media upload object.\n",
        "media = MediaFileUpload('/content/script.txt')\n",
        "\n",
        "# Create upload request.\n",
        "upload_request = service.media().upload(\n",
        "    resourceName=custom_bidding_script_ref['resourceName'], media_body=media)\n",
        "\n",
        "# Override response handler to expect null response.\n",
        "upload_request.postproc = HttpRequest.null_postproc\n",
        "\n",
        "# Upload script to resource location given in retrieved custom bidding\n",
        "# script reference object.\n",
        "upload_request.execute()\n",
        "\n",
        "# Create a custom bidding script object.\n",
        "script_obj = {\n",
        "    'script': custom_bidding_script_ref\n",
        "}\n",
        "\n",
        "# Create the custom bidding script.\n",
        "response = service.customBiddingAlgorithms().scripts().create(\n",
        "    customBiddingAlgorithmId=algorithmId,\n",
        "    advertiserId=advertiserId,\n",
        "    body=script_obj).execute()\n",
        "\n",
        "# Display the new custom bidding script object.\n",
        "print(f'The following custom bidding script was created: {response}')\n",
        "\n",
        "# Create the new bid strategy object.\n",
        "bidding_strategy = {\n",
        "    'maximizeSpendAutoBid': {\n",
        "        'performanceGoalType':\n",
        "            'BIDDING_STRATEGY_PERFORMANCE_GOAL_TYPE_CUSTOM_ALGO',\n",
        "        'customBiddingAlgorithmId': algorithmId\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create a line item object assigning the new bid strategy.\n",
        "line_item_obj = {'bidStrategy': bidding_strategy}\n",
        "\n",
        "# Update the line item with a new bid strategy.\n",
        "#response = service.advertisers().lineItems().patch(\n",
        "#    advertiserId=advertiserId,\n",
        "#    lineItemId=line-item-id,\n",
        "#    updateMask='bidStrategy',\n",
        "#    body=line_item_obj).execute()\n",
        "\n",
        "# Display the line item's new bid strategy\n",
        "#print(f'Line Item {response[\"name\"]} is now using the following bid'\n",
        "#     f' strategy: {response[\"bidStrategy\"]}.')"
      ],
      "metadata": {
        "id": "EekvlHfUbpQb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}